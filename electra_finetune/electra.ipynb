{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9857159f-25d2-4cc0-9d67-73a85cb5bc4a",
   "metadata": {},
   "source": [
    "# Electra Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e1f80-218f-4ae1-8ad8-46ea33027de3",
   "metadata": {},
   "source": [
    "### Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47af020a-d91f-48df-b035-d17b6efacd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c8336-672b-45ee-838a-6eb0073b81b7",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8057d9-4097-457e-92ee-f50d1b576764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hugging Face 데이터셋 로드 (sentence-transformers/all-nli, subset: triplet)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# \"sentence-transformers/all-nli\" 데이터셋의 \"triplet\" 설정을 불러옵니다.\n",
    "dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\n",
    "train_data = dataset[\"train\"]\n",
    "dev_data = dataset[\"dev\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac0221f-b923-4263-aa4b-1a94c4c7bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 데이터 예시:\n",
      " {'anchor': 'A person on a horse jumps over a broken down airplane.', 'positive': 'A person is outdoors, on a horse.', 'negative': 'A person is at a diner, ordering an omelette.'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예시: anchor, positive, negative 세 가지 문장이 들어 있습니다.\n",
    "print(\"샘플 데이터 예시:\\n\", train_data[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c561b6-086d-47f6-b4fd-5563bab5b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.select(range(10000))  # 처음 10000개만 사용 (옵션)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcdaed-3c91-48c8-988e-670e3e65649e",
   "metadata": {},
   "source": [
    "### Init Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2696d968-2b3e-4b29-a6a9-e1bdcd7e2dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c23f9ecaf1a4b71962404a4c41b810b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357ee383c9b64e638d2565ec2518a9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db30f886f4e46139609ed3c355b36cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1131423bfdc42deb59f48f36544eb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db02a45897840dc8d718e71c576e519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 device: cuda\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc485e19d6574b8fa60833271cd8c429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. 모델과 토크나이저 초기화 (ELECTRA 기반 모델 사용)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"google/electra-small-discriminator\"  # ELECTRA Small (더 큰 모델은 \"google/electra-base-discriminator\" 등 사용 가능)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# CPU/GPU 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"사용 중인 device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd4e6c-aa19-4708-b78e-2a563d3e7e6f",
   "metadata": {},
   "source": [
    "### Define Loss-func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf1009f-c038-4c5d-a45e-fe93e070d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 손실 함수와 옵티마이저 설정 (TripletMarginLoss 및 AdamW)\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)  # margin=1.0으로 Triplet Margin Loss 정의\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # AdamW 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12593047-66af-4077-994f-9b78d3a5d409",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffa7420c-bd45-4b33-81db-9d90fd29aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 훈련 루프 정의\n",
    "def train_triplet(model, dataset, epochs=1, batch_size=32, log_interval=200):\n",
    "    model.train()  # 모델을 훈련 모드로 전환\n",
    "    for epoch in range(epochs):\n",
    "        # 각 epoch마다 데이터를 섞어서 순서 무작위화\n",
    "        dataset = dataset.shuffle(seed=epoch)\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch = dataset[i : i+batch_size]\n",
    "            anchors = batch[\"anchor\"]\n",
    "            positives = batch[\"positive\"]\n",
    "            negatives = batch[\"negative\"]\n",
    "            \n",
    "            # 입력 문장들을 토크나이징하여 텐서로 변환 (padding 및 truncation 적용)\n",
    "            enc_anchors = tokenizer(anchors, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            enc_pos = tokenizer(positives, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            enc_neg = tokenizer(negatives, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            # device(cpu/gpu)로 tensors 이동\n",
    "            enc_anchors = {key: val.to(device) for key, val in enc_anchors.items()}\n",
    "            enc_pos = {key: val.to(device) for key, val in enc_pos.items()}\n",
    "            enc_neg = {key: val.to(device) for key, val in enc_neg.items()}\n",
    "            \n",
    "            # 모델 forward 수행하여 각 입력의 hidden state 얻기\n",
    "            out_anchor = model(**enc_anchors)\n",
    "            out_positive = model(**enc_pos)\n",
    "            out_negative = model(**enc_neg)\n",
    "            \n",
    "            # 문장 임베딩으로 [CLS] 토큰의 hidden state를 사용 (Electra는 pooler 없음)\n",
    "            anchor_emb = out_anchor.last_hidden_state[:, 0, :]\n",
    "            positive_emb = out_positive.last_hidden_state[:, 0, :]\n",
    "            negative_emb = out_negative.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            # Triplet margin 손실 계산\n",
    "            loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            # 역전파 및 모델 업데이트\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # 일정 간격으로 진행 상황 출력\n",
    "            if (i // batch_size) % log_interval == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {i//batch_size}, Loss: {loss.item():.4f}\")\n",
    "        avg_loss = total_loss / (len(dataset) // batch_size + 1)\n",
    "        print(f\"--> Epoch {epoch+1} 완료 (평균 손실: {avg_loss:.4f})\\n\")\n",
    "\n",
    "    # ---- 학습 완료 후 모델 가중치 저장 ----\n",
    "    # 학습된 모델의 state_dict()만 저장합니다.\n",
    "    model_weights_path = \"triplet_model_weights.pt\"\n",
    "    torch.save(model.state_dict(), model_weights_path)\n",
    "    print(f\"모델의 가중치를 '{model_weights_path}' 파일로 저장하였습니다.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c24ec2f7-b078-40a0-8147-b5be6c4e4e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 0.1649\n",
      "Epoch 1, Step 200, Loss: 0.2069\n",
      "--> Epoch 1 완료 (평균 손실: 0.1655)\n",
      "\n",
      "Epoch 2, Step 0, Loss: 0.0342\n",
      "Epoch 2, Step 200, Loss: 0.1028\n",
      "--> Epoch 2 완료 (평균 손실: 0.0938)\n",
      "\n",
      "Epoch 3, Step 0, Loss: 0.0000\n",
      "Epoch 3, Step 200, Loss: 0.0194\n",
      "--> Epoch 3 완료 (평균 손실: 0.0626)\n",
      "\n",
      "Epoch 4, Step 0, Loss: 0.0787\n",
      "Epoch 4, Step 200, Loss: 0.1346\n",
      "--> Epoch 4 완료 (평균 손실: 0.0420)\n",
      "\n",
      "Epoch 5, Step 0, Loss: 0.0550\n",
      "Epoch 5, Step 200, Loss: 0.0931\n",
      "--> Epoch 5 완료 (평균 손실: 0.0329)\n",
      "\n",
      "모델의 가중치를 'triplet_model_weights.pt' 파일로 저장하였습니다.\n"
     ]
    }
   ],
   "source": [
    "train_triplet(model, train_data, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d977ea-13af-42b7-a296-4c846d26fef2",
   "metadata": {},
   "source": [
    "### Inference (Simple Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb5fc797-1a8c-4058-9170-073d00918a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 모델 가중치를 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "# ===== 추론을 위한 모델 불러오기 =====\n",
    "# 같은 구조의 모델을 사전학습 가중치 기반으로 초기화합니다.\n",
    "inference_model = AutoModel.from_pretrained(model_name)\n",
    "# 저장된 학습 가중치를 불러와서 모델에 적용합니다.\n",
    "inference_model.load_state_dict(torch.load(\"triplet_model_weights.pt\"))\n",
    "# 모델을 장치(device)로 이동하고 평가 모드로 전환합니다.\n",
    "inference_model.to(device)\n",
    "inference_model.eval()\n",
    "print(\"저장된 모델 가중치를 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007cf132-9c4c-42b2-b7c4-ca5efab7bb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: '이것은 테스트 문장입니다.'\n",
      "추론 결과 임베딩 벡터 크기: (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# 추론(inference) 함수 정의\n",
    "def infer_sentence(sentence: str):\n",
    "    \"\"\"단일 문장에 대한 추론을 수행하고, 임베딩 벡터를 반환합니다.\"\"\"\n",
    "    # 입력 문장을 토크나이즈하여 텐서로 변환\n",
    "    encoding = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "    # 추론 단계에서는 gradient 계산 불필요(torch.no_grad() 사용)\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**encoding)\n",
    "        # CLS 토큰의 은닉 상태를 임베딩으로 추출\n",
    "        sentence_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return sentence_emb\n",
    "\n",
    "# 추론 예시\n",
    "test_sentence = \"이것은 테스트 문장입니다.\"\n",
    "result_vector = infer_sentence(test_sentence)\n",
    "print(f\"입력 문장: '{test_sentence}'\")\n",
    "print(f\"추론 결과 임베딩 벡터 크기: {result_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67215722-5b11-4f38-903a-54d86499dbca",
   "metadata": {},
   "source": [
    "### Inference (Most Similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1399748e-ae57-4086-901f-b405eb6e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def find_most_similar(model, anchor_text, candidate_texts):\n",
    "    model.eval()  # 평가 모드 (dropout 비활성화 등)\n",
    "    # 토크나이즈 및 텐서 변환\n",
    "    enc_anchor = tokenizer(anchor_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    enc_candidates = tokenizer(candidate_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Gradient 계산 비활성화 (inference)\n",
    "    with torch.no_grad():\n",
    "        anchor_output = model(**enc_anchor)\n",
    "        cand_output = model(**enc_candidates)\n",
    "    # 임베딩 추출 ([CLS] 토큰의 hidden state 사용)\n",
    "    anchor_emb = anchor_output.last_hidden_state[:, 0, :]\n",
    "    cand_emb = cand_output.last_hidden_state[:, 0, :]\n",
    "    # 코사인 유사도 계산\n",
    "    anchor_emb = F.normalize(anchor_emb, p=2, dim=1)  # 정규화 (크기 1의 벡터)\n",
    "    cand_emb = F.normalize(cand_emb, p=2, dim=1)\n",
    "    similarities = torch.matmul(cand_emb, anchor_emb.T).squeeze(1)  # 각 후보와 앵커의 코사인 유사도\n",
    "    best_idx = int(torch.argmax(similarities))  # 가장 높은 유사도의 인덱스\n",
    "    best_sentence = candidate_texts[best_idx]\n",
    "    return best_idx, best_sentence, similarities.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc1ddc08-afe6-4bbb-80cc-a26fed106696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor 문장: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "후보 문장들: ['The church is filled with song.', 'A choir singing at a baseball game.', 'The woman has been shot.']\n"
     ]
    }
   ],
   "source": [
    "# 앵커와 후보 문장을 설정합니다. (여기서는 테스트 세트의 첫 번째 예시를 사용합니다.)\n",
    "anchor_example = test_data[0][\"anchor\"]\n",
    "positive_example = test_data[0][\"positive\"]\n",
    "negative_example = test_data[0][\"negative\"]\n",
    "other_example = test_data[1][\"negative\"]  # 다른 무작위 문장 (negative 예시로 사용)\n",
    "\n",
    "candidate_sentences = [positive_example, negative_example, other_example]\n",
    "\n",
    "print(\"Anchor 문장:\", anchor_example)\n",
    "print(\"후보 문장들:\", candidate_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0c104e-4fbf-4f86-a9ac-a6871fcfd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후보[0] 유사도: 0.8633 | 문장: The church is filled with song.\n",
      "후보[1] 유사도: 0.8732 | 문장: A choir singing at a baseball game.\n",
      "후보[2] 유사도: 0.9052 | 문장: The woman has been shot.\n",
      "=> 가장 유사한 문장 (예측 Positive): 후보[2] \"The woman has been shot.\"\n"
     ]
    }
   ],
   "source": [
    "# 가장 유사한 문장 찾기\n",
    "best_idx, best_sentence, sims = find_most_similar(model, anchor_example, candidate_sentences)\n",
    "\n",
    "# 결과 출력\n",
    "for idx, (cand, sim) in enumerate(zip(candidate_sentences, sims)):\n",
    "    print(f\"후보[{idx}] 유사도: {sim:.4f} | 문장: {cand}\")\n",
    "print(f\"=> 가장 유사한 문장 (예측 Positive): 후보[{best_idx}] \\\"{best_sentence}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1379c27-fb3c-45b9-8d22-bdfb57eb8c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
